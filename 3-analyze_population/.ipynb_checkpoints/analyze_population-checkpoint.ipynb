{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate population parameters\n",
    "\n",
    "...by defining a merger-rate model and using all the events' data to constrain it.\n",
    "\n",
    "Notation:\n",
    "* $p = \\{\\mathcal{M}, q, \\chi_{\\rm eff}, \\ldots \\}$ are the physical parameters of the merger, `p_params`.\n",
    "* $\\mu$ are the population-model parameters, `mu_params`.\n",
    "* $R(p, \\mu)$ are modeled universe-rates, so that $R(p, \\mu){\\rm d}p$ has units of $(VT)^{-1}$.\n",
    "* $\\lambda(p, \\mu) \\approx V(p) R(p, \\mu)$ are the detector-rates, where $V(p)$ is the detector network sensitive volume averaged over angles ($\\sim\\propto {\\rm SNR_{1Mpc}^3}$). The actual cimputation accounts for cosmology.\n",
    "* $\\overline\\lambda(\\mu) = \\int {\\rm d}p V(p) R(p, \\mu)$ is the total detector-rate.\n",
    "* $r(p, \\mu) = \\lambda(p, \\mu) / \\overline\\lambda(\\mu)$ are the *relative* detector-rates, normalized so that $\\int r(p, \\mu){\\rm d}p = 1$.\n",
    "* $\\mathcal{L}_i(p)$ is the likelihood that the $i$-th GW event has parameters $p$, normalized to $\\int \\mathcal{L}_i(p){\\rm d}p = 1$.\n",
    "\n",
    "$$P({\\rm data \\mid model}, \\mu) \\propto \\prod_{i \\in \\rm \\{events\\}} \\int {\\rm d}p ~ r(p, \\mu) \\mathcal{L}_i(p)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndimage\n",
    "from scipy.interpolate import UnivariateSpline, interp1d\n",
    "from scipy.optimize import brentq as brentq\n",
    "from scipy.special import erf\n",
    "from numpy import sqrt, pi\n",
    "from collections import OrderedDict\n",
    "from itertools import repeat\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = ['GW150914',\n",
    "          'GW151226',\n",
    "          'LVT151012',\n",
    "          'GW170104',\n",
    "          'GW170608',\n",
    "          'GW170814'\n",
    "         ]\n",
    "\n",
    "approximants = [#'IMRPhenomD',\n",
    "                'SEOBNRv4_ROM'\n",
    "               ]\n",
    "PSDs = [#'ASD_zdhp',\n",
    "        'ASD_average',\n",
    "       ]\n",
    "coherences = ['coherent',\n",
    "              #'incoherent',\n",
    "             ]\n",
    "SNR_thresh = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PSDs_coherences_approximants = [(PSD, coherence, approximant) for PSD in PSDs \n",
    "                                                              for coherence in coherences\n",
    "                                                              for approximant in approximants]\n",
    "\n",
    "coherences_approximants = [(coherence, approximant) for coherence in coherences \n",
    "                                                    for approximant in approximants]\n",
    "\n",
    "PSDs_approximants = [(psd, approximant) for psd in PSDs\n",
    "                     for approximant in approximants]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "* Grid in $p$-space for each event $i$, including $\\log \\mathcal{L}_i(p)$  \n",
    "Compute the likelihood, normalize to $\\int \\mathcal{L}_i(p){\\rm d}p = 1$.\n",
    "\n",
    "* Sensitive volume $V(p) \\propto {\\rm SNR}_{1 \\rm Mpc}^3$ on a full $p$-grid  \n",
    "Interpolate $V(p)$ on the $p$-space grids of each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_grid = {}\n",
    "metadata = {}\n",
    "for event in events:\n",
    "    p_grid[event] = pd.read_csv('../1-estimate_parameters/{}/parameter_grid'.format(event), sep='\\s+')\n",
    "    for CA in coherences_approximants:\n",
    "        L = np.exp(p_grid[event]['logL_' + '_'.join(CA)])\n",
    "        L /= L.sum()\n",
    "        if event == 'LVT151012':\n",
    "            L = .87*L + .13/np.size(L)  # Marginalize over the possibility that LVT is noise\n",
    "        p_grid[event]['likelihood_' + '_'.join(CA)] = L\n",
    "        del p_grid[event]['logL_' + '_'.join(CA)]\n",
    "    metadata[event] = pd.read_csv('../1-estimate_parameters/{}/grid_metadata'.format(event), sep='\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load SNR_1Mpc on full p-grid\n",
    "p_params = ['M_chirp', 'q', 'chi_eff']\n",
    "p_grid['full'] = pd.read_csv('../2-estimate_horizon/all_par_space/parameter_grid', sep='\\s+')\n",
    "metadata['full'] = pd.read_csv('../2-estimate_horizon/all_par_space/grid_metadata', sep='\\s+')\n",
    "\n",
    "assert all(p_par in grid for p_par in p_params for grid in p_grid.values())\n",
    "assert all(all(mdata.columns == p_params) for mdata in metadata.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_grid_1d = {key: {par: np.sort(list(set(p_grid[key][par].values))) for par in p_params} for key in p_grid}\n",
    "# Transform p_grid in dictionaries of arrays with actual grid shape\n",
    "for key in p_grid:\n",
    "    p_grid[key] = {k: p_grid[key][k].values.reshape(metadata[key].values[0]) for k in p_grid[key]}\n",
    "p_grid_1d['u'] = np.linspace(0, 1, 16)  # u = dL / dL_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Interpolate over each event's grid, and also refine the full p_grid => p_igrid\n",
    "min_val, max_val = ({p_par: f(p_grid['full'][p_par]) for p_par in p_params} for f in (np.min, np.max))\n",
    "zoom = OrderedDict([('M_chirp', 4), ('q', 8), ('chi_eff', 8)])\n",
    "p_igrid_1d = {p_par: np.linspace(min_val[p_par], max_val[p_par],\n",
    "                                 zoom[p_par] * metadata['full'][p_par][0]) for p_par in p_params}\n",
    "p_igrid = OrderedDict(zip(p_params, np.meshgrid(*[p_igrid_1d[p_par] for p_par in p_params], indexing='ij')))\n",
    "for PA in PSDs_approximants:\n",
    "    SNR_1Mpc = p_grid['full']['SNR_1Mpc_' + '_'.join(PA)]\n",
    "    for event in events:\n",
    "        p_grid[event]['SNR_1Mpc_' + '_'.join(PA)] = ndimage.map_coordinates(\n",
    "            SNR_1Mpc, [(p_grid[event][p_par] - min_val[p_par]) * (\n",
    "                (metadata['full'][p_par][0]-1) / (max_val[p_par]-min_val[p_par])) for p_par in p_params])\n",
    "    p_igrid[(*PA), 'SNR_1Mpc'] = ndimage.zoom(SNR_1Mpc, (zoom[p_par] for p_par in p_params), order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosmological relations\n",
    "Luminosity distance as a function of redshift and viceversa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_over_H0 = 4422  # Mpc\n",
    "Omega_m = 0.308\n",
    "\n",
    "# From Adachi M., Kasai M., 2012, Progress of Theoretical Physics, 127, 145\n",
    "def x(z):\n",
    "    return (1 - Omega_m) / (Omega_m*(1 + z)**3)\n",
    "def phi(x):\n",
    "    return ( (1 + 1.32*x + 0.4415*x**2 + 0.02656*x**3)\n",
    "            /(1 + 1.392*x + .5121*x**2 + 0.03944*x**3))\n",
    "def dL(z):  # Luminosity distance\n",
    "    return (2*c_over_H0 * (1+z) / np.sqrt(Omega_m) \n",
    "            * (phi(x(0)) - phi(x(z)) / np.sqrt(1+z)))\n",
    "zs = np.linspace(0, 2)\n",
    "z_of_DL = interp1d(dL(zs), zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the probability $P(w > w_\\ast)$ that a random orientation and sky location will have a weight factor $w$ greater than $w_\\ast$, as a function of $w_\\ast$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_points, P_gr_w_points = np.loadtxt('P_greater_than_w.dat', unpack=True)\n",
    "P_gr_w = interp1d(w_points, P_gr_w_points, kind='cubic', bounds_error=False, fill_value=(np.nan, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model physical rates\n",
    "\n",
    "Define some rate models, that return the physical rate $R(p, \\mu)$. The only important information is the ratio $R(p_1, \\mu) / R(p_2,\\mu)$ for the same $\\mu$. Normalization is otherwise arbitrary.\n",
    "\n",
    "### Models on spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_chi_eff(chi_eff, alpha, **kwargs):\n",
    "    return np.exp(alpha * chi_eff)\n",
    "simple_chi_eff.latex = r'$R(\\chi_{\\rm eff}) \\propto e^{\\alpha \\chi_{\\rm eff}}$'\n",
    "simple_chi_eff.p_params = ['chi_eff']\n",
    "simple_chi_eff.mu_params = ['alpha']\n",
    "simple_chi_eff.mu_bounds = {'alpha': [-5, 5]}\n",
    "\n",
    "def exponential_chi_eff(chi_eff, avg_chi_eff, **kwargs):\n",
    "    abs_avg = abs(avg_chi_eff)\n",
    "    alpha = (np.sign(avg_chi_eff) \n",
    "             * brentq(lambda a: 1/np.tanh(a) - 1/a - abs_avg, 3*abs_avg, 3 * (1/(1-abs_avg) - 1)))\n",
    "    assert(np.sign(avg_chi_eff) == np.sign(alpha))\n",
    "    return np.exp(alpha * chi_eff)\n",
    "exponential_chi_eff.latex = r'$R(\\chi_{\\rm eff}) \\propto e^{\\alpha \\chi_{\\rm eff}}$'\n",
    "exponential_chi_eff.p_params = ['chi_eff']\n",
    "exponential_chi_eff.mu_params = ['avg_chi_eff']\n",
    "exponential_chi_eff.mu_bounds = {'avg_chi_eff': [-.99, .99]}\n",
    "\n",
    "def both_spinning(q, chi_eff, alpha1, alpha2, **kwargs):\n",
    "    # A rustic but fast piecewise function:\n",
    "    cond1 = 1 + chi_eff + q*chi_eff <= q\n",
    "    cond2 = q + chi_eff + q*chi_eff > 1\n",
    "    cond3 = ~ (cond1 | cond2)  # Neither\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        R = np.divide( \n",
    "            cond1 * (np.exp(-alpha1 - alpha2)*(np.exp((1 + q)*alpha1*(1 + chi_eff))\n",
    "                     - np.exp(((1 + q)*alpha2*(1 + chi_eff))/q))*(1 + q))\n",
    "            + cond2 * (np.exp(-alpha1 - alpha2) * (-np.exp(2*alpha2 + alpha1*(1 + q*(-1 + chi_eff) + chi_eff))\n",
    "                       +np.exp(2*alpha1 + (alpha2*(-1 + q + chi_eff + q*chi_eff))/q))*(1+q))\n",
    "            + cond3 * (2*np.exp((1 + q)*alpha1*chi_eff)*(1 + q)*np.sinh(q*alpha1 - alpha2))\n",
    "            , (q*alpha1 - alpha2))\n",
    "    R[np.isinf(R) | np.isnan(R)] = 0  # Exclude nans and infs\n",
    "    return R\n",
    "both_spinning.latex = r'$R(\\chi_i) \\propto e^{\\alpha_i \\chi_i}$'\n",
    "both_spinning.p_params = ['q', 'chi_eff']\n",
    "both_spinning.mu_params = ['alpha1', 'alpha2']\n",
    "both_spinning.mu_bounds = {'alpha1': [-5, 5],\n",
    "                           'alpha2': [-5, 5]}\n",
    "\n",
    "def zero_mean_gaussian_chi_eff(chi_eff, std_chi_eff, **kwargs):\n",
    "    return np.exp(-.5 * chi_eff**2 / std_chi_eff**2)\n",
    "zero_mean_gaussian_chi_eff.latex = (r'$R(\\chi_{\\rm eff}) \\propto '\n",
    "                                    r'\\exp(-\\chi_{\\rm eff}^2 / 2 \\sigma_{\\chi_{\\rm eff}}^2)}$')\n",
    "zero_mean_gaussian_chi_eff.p_params = ['chi_eff']\n",
    "zero_mean_gaussian_chi_eff.mu_params = ['std_chi_eff']\n",
    "zero_mean_gaussian_chi_eff.mu_bounds = {'std_chi_eff': [5e-3, .5]}\n",
    "\n",
    "def gaussian_chi_eff(chi_eff, mean_chi_eff, std_chi_eff, **kwargs):\n",
    "    return np.exp(-(chi_eff - mean_chi_eff)**2 / std_chi_eff**2 / 2)\n",
    "gaussian_chi_eff.latex = (r'$R(\\chi_{\\rm eff}) \\propto \\exp \\left('\n",
    "    r'-(\\chi_{\\rm eff}-\\overline{\\chi}_{\\rm eff})^2 / 2 \\sigma_{\\chi_{\\rm eff}}^2 \\right)$')\n",
    "gaussian_chi_eff.p_params = ['chi_eff']\n",
    "gaussian_chi_eff.mu_params = ['mean_chi_eff', 'std_chi_eff']\n",
    "gaussian_chi_eff.mu_bounds = {'mean_chi_eff': [-.3, .3],\n",
    "                              'std_chi_eff': [5e-3, .45]}\n",
    "\n",
    "def gaussian_2ndlocked_chi_eff(q, chi_eff, std_chi_eff, f, **kwargs):\n",
    "    '''A zero-mean gaussian population * (1-f), plus a secondary-locked population * f.'''\n",
    "    return ((1-f) * np.exp(-chi_eff**2 / std_chi_eff**2 / 2)\n",
    "            + f * sqrt(1+q**2) * np.exp(-(chi_eff-q/(1+q))**2 * (1+q**2) / std_chi_eff**2 / 2))\n",
    "gaussian_2ndlocked_chi_eff.latex = (r'$R(\\chi_{\\rm eff}, q) \\propto'\n",
    "    r'(1-f) G(\\chi_{\\rm eff}, \\sigma_{\\chi_{\\rm eff}})'\n",
    "    r'+ f \\, G(\\chi_{\\rm eff} - \\frac{q}{1+q}, \\frac{\\sigma_{\\chi_{\\rm eff}}}{\\sqrt{1+q^2}})$')\n",
    "gaussian_2ndlocked_chi_eff.p_params = ['q', 'chi_eff']\n",
    "gaussian_2ndlocked_chi_eff.mu_params = ['f', 'std_chi_eff']\n",
    "gaussian_2ndlocked_chi_eff.mu_bounds = {'f': [0, 1],\n",
    "                                        'std_chi_eff': [1e-2, .4]}\n",
    "\n",
    "def a_bar_random_angle(q, chi_eff, a_bar, **kwargs):\n",
    "    a = np.maximum(((1+q)*chi_eff - a_bar)/q, -a_bar)\n",
    "    b = np.minimum(((1+q)*chi_eff + a_bar)/q, a_bar)\n",
    "    return (1+q)*(b-a) * (b-a > 0)\n",
    "a_bar_random_angle.latex = r'$R(\\chi_{1, 2}) = U(-\\overline{a}, \\overline{a})$'\n",
    "a_bar_random_angle.p_params = ['q', 'chi_eff']\n",
    "a_bar_random_angle.mu_params = ['a_bar']\n",
    "a_bar_random_angle.mu_bounds = {'a_bar': [1e-2, 1]}\n",
    "\n",
    "def a_bar_mu_avg(q, chi_eff, a_bar, mu_avg, **kwargs):\n",
    "    mu_min = 2*mu_avg - 1\n",
    "    a = np.maximum(((1+q)*chi_eff - a_bar)/q, mu_min*a_bar)\n",
    "    b = np.minimum(((1+q)*chi_eff - mu_min*a_bar)/q, a_bar)\n",
    "    return (1+q)*(b-a) * (b-a > 0)\n",
    "a_bar_mu_avg.latex = r'$R(\\chi_{1, 2}) = U(\\mu_{\\rm min}\\overline{a}, \\overline{a})$'\n",
    "a_bar_mu_avg.p_params = ['q', 'chi_eff']\n",
    "a_bar_mu_avg.mu_params = ['a_bar', 'mu_avg']\n",
    "a_bar_mu_avg.mu_bounds = {'a_bar': [1e-2, .85],\n",
    "                          'mu_avg': [0, .85]}\n",
    "\n",
    "def a_bar_equal_angles(chi_eff, a_bar, **kwargs):\n",
    "    return np.abs(chi_eff) < a_bar\n",
    "a_bar_equal_angles.latex = r'$R(\\chi_{\\rm eff}) = U(-\\overline{a}, \\overline{a})$'\n",
    "a_bar_equal_angles.p_params = ['chi_eff']\n",
    "a_bar_equal_angles.mu_params = ['a_bar']\n",
    "a_bar_equal_angles.mu_bounds = {'a_bar': [1e-2, .85]}\n",
    "\n",
    "def a_bar_mu_avg_equal_angles(chi_eff, a_bar, mu_avg, **kwargs):\n",
    "    mu_min = 2*mu_avg - 1\n",
    "    return (chi_eff > mu_min*a_bar) & (chi_eff < a_bar)\n",
    "a_bar_mu_avg_equal_angles.latex = r'$R(\\chi_{\\rm eff}) = U(\\mu_{\\rm min}\\overline{a}, \\overline{a})$'\n",
    "a_bar_mu_avg_equal_angles.p_params = ['chi_eff']\n",
    "a_bar_mu_avg_equal_angles.mu_params = ['a_bar', 'mu_avg']\n",
    "a_bar_mu_avg_equal_angles.mu_bounds = {'a_bar': [1e-2, .85],\n",
    "                          'mu_avg': [0, .85]}\n",
    "\n",
    "def heavy_spinning(chi_eff, q, a_bar, **kwargs):\n",
    "    return (1+q) * ((1+q)*np.abs(chi_eff) < a_bar)\n",
    "heavy_spinning.latex = r'$R(\\chi_1) = U(-\\overline{a}, \\overline{a})$'\n",
    "heavy_spinning.p_params = ['q', 'chi_eff']\n",
    "heavy_spinning.mu_params = ['a_bar']\n",
    "heavy_spinning.mu_bounds = {'a_bar': [5e-2, .85]}\n",
    "\n",
    "def P_chi_neg_mumin(chi, mu_min, a_min, a_max):\n",
    "    return np.log(a_max / np.minimum(a_max, np.maximum(\n",
    "        a_min, np.maximum(chi/mu_min, chi))))\n",
    "def P_chi_pos_mumin(chi, mu_min, a_min, a_max):\n",
    "    return np.log(np.maximum(a_min, np.minimum(a_max, chi/mu_min))\n",
    "                / np.minimum(a_max, np.maximum(a_min, chi)))\n",
    "def _int_dchi2(q, chi_eff, a_min, a_max, mu_min):  # args must be floats\n",
    "    '''Returns int dchi2 Pchi(chi1(chi2)) Pchi(chi2)'''\n",
    "    chi_min = min(mu_min*a_min, mu_min*a_max)\n",
    "    chi_max = a_max  # mu_max == 1\n",
    "    chi2_min = min(a_max, max(chi_min, ((1+q)*chi_eff-chi_max) / q))\n",
    "    chi2_max = max(a_min, min(chi_max, ((1+q)*chi_eff-chi_min) / q))\n",
    "    chi2, dchi2 = np.linspace(chi2_min, chi2_max, 64, retstep=True)\n",
    "    P_chi = [P_chi_neg_mumin, P_chi_pos_mumin][mu_min > 0]\n",
    "    return (P_chi((1+q)*chi_eff - q*chi2, mu_min, a_min, a_max)\n",
    "          * P_chi(chi2, mu_min, a_min, a_max)).sum() * dchi2\n",
    "def a_star_delta_a_mu_min(q, chi_eff, a_star, mu_avg, delta_a=.1, **kwargs):\n",
    "    mu_min = 2*mu_avg - 1\n",
    "    a_min = max(1e-2, a_star - delta_a)\n",
    "    a_max = min(1, a_star + delta_a)\n",
    "    p = Pool(cpu_count())\n",
    "    int_dchi2 = p.starmap(_int_dchi2, list(zip(\n",
    "        np.array(q).flatten(), np.array(chi_eff).flatten(), repeat(a_min), repeat(a_max), repeat(mu_min))))\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return (1+q) * np.reshape(int_dchi2, np.array(q).shape)\n",
    "a_star_delta_a_mu_min.latex = r'$R(a_{1,2}, \\mu_{1,2}) = U(a_\\ast \\pm 0.1) U(\\mu_{\\rm min}, 1)$'\n",
    "a_star_delta_a_mu_min.p_params = ['q', 'chi_eff']\n",
    "a_star_delta_a_mu_min.mu_params = ['a_star', 'mu_avg']\n",
    "a_star_delta_a_mu_min.mu_bounds = {'a_star': [0, .85],\n",
    "                                   'mu_avg': [0, .98]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models on mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def M_chirp_power_law_cutoff(M_chirp, alpha, Mchirp_max, Mchirp_min=5, **kwargs):\n",
    "    return M_chirp**-alpha * (5 < M_chirp) * (M_chirp < Mchirp_max)\n",
    "M_chirp_power_law_cutoff.latex = (r'$R(\\mathcal{{M}}_s) \\propto \\mathcal{{M}}_s^{{-\\alpha}}$, '\n",
    "                                  r'$\\mathcal{{M}}_s \\in (5 M_\\odot, \\mathcal{{M}}_{{\\rm max}})$')\n",
    "M_chirp_power_law_cutoff.p_params = ['M_chirp']\n",
    "M_chirp_power_law_cutoff.mu_params = ['alpha', 'Mchirp_max']\n",
    "M_chirp_power_law_cutoff.mu_bounds = {'alpha': [-3, 5],\n",
    "                                      'Mchirp_max': [15, 100]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models on mass ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_power_law(q, q_avg, **kwargs):\n",
    "    return q ** ((2*q_avg-1) / (1-q_avg))\n",
    "q_power_law.latex = r'$R(q) \\propto q^\\alpha$'\n",
    "q_power_law.p_params = ['q']\n",
    "q_power_law.mu_params = ['q_avg']\n",
    "q_power_law.mu_bounds = {'q_avg': [0, .975]}\n",
    "\n",
    "def q_half_gaussian(q, std_q, **kwargs):\n",
    "    return np.exp(-(q-1)**2 /2 /std_q**2)\n",
    "q_half_gaussian.latex = r'$R(q) \\propto \\exp(-(q-1)^2/2\\sigma_q^2)$'\n",
    "q_half_gaussian.p_params = ['q']\n",
    "q_half_gaussian.mu_params = ['std_q']\n",
    "q_half_gaussian.mu_bounds = {'std_q': [1e-2, 2]}\n",
    "\n",
    "\n",
    "def q_power_law_chi_eff_gaussian(q, chi_eff, q_avg, std_chi_eff, **kwargs):\n",
    "    return q ** ((2*q_avg-1) / (1-q_avg)) * np.exp(-chi_eff**2 /2/std_chi_eff**2)\n",
    "q_power_law_chi_eff_gaussian.latex = (r'$R(q, \\chi_{\\rm eff}) \\propto q^\\alpha '\n",
    "                                      r'G(\\chi_{\\rm eff}, \\sigma_chi_{\\rm eff})$')\n",
    "q_power_law_chi_eff_gaussian.p_params = ['q', 'chi_eff']\n",
    "q_power_law_chi_eff_gaussian.mu_params = ['q_avg', 'std_chi_eff']\n",
    "q_power_law_chi_eff_gaussian.mu_bounds = {'q_avg': [0, .975],\n",
    "                                          'std_chi_eff': [1e-2, .5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latex = {'M_chirp': r'$\\mathcal{{M}}$',\n",
    "         'q': r'$q$',\n",
    "         'chi_eff': r'$\\chi_{{\\rm eff}}$',\n",
    "         'alpha': r'$\\alpha$',\n",
    "         'beta': r'$\\beta$',\n",
    "         'alpha1': r'$\\alpha_1$',\n",
    "         'alpha2': r'$\\alpha_2$',\n",
    "         'Mchirp_max': r'$\\mathcal{{M}}_{{\\rm max}}$',\n",
    "         'mean_chi_eff': r'$\\overline{{\\chi}}_{{\\rm eff}}$',\n",
    "         'std_chi_eff': r'$\\sigma_{{\\chi_{{\\rm eff}}}}$',\n",
    "         'f': '$f$',\n",
    "         'q_avg': r'$\\overline{{q}}$',\n",
    "         'avg_chi_eff': r'$\\langle \\chi_{{\\rm eff}} \\rangle$',\n",
    "         'std_q': r'$\\sigma_q$',\n",
    "         'a_bar': r'$\\overline{{a}}$',\n",
    "         'mu_avg': r'$\\overline{{\\mu}}$',\n",
    "         'a_star': r'$a_\\ast$'\n",
    "        }\n",
    "unit = {par: r' (${\\rm M}_\\odot$)' if par in ['M_chirp', 'Mchirp_max'] else '' for par in latex}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "#           simple_chi_eff,\n",
    "#           exponential_chi_eff,\n",
    "#           both_spinning,\n",
    "          M_chirp_power_law_cutoff,\n",
    "          zero_mean_gaussian_chi_eff,\n",
    "#           gaussian_chi_eff,\n",
    "#           gaussian_2ndlocked_chi_eff,\n",
    "           q_power_law,\n",
    "#           q_half_gaussian,\n",
    "#           a_bar_mu_avg,\n",
    "#           a_bar_random_angle,\n",
    "#           a_bar_equal_angles,\n",
    "#           a_bar_mu_avg_equal_angles,\n",
    "#           heavy_spinning,\n",
    "#           q_power_law_chi_eff_gaussian,\n",
    "          a_star_delta_a_mu_min,\n",
    "         ]\n",
    "assert all(par in p_params for model in models for par in model.p_params)\n",
    "for model in models:\n",
    "    model.p_params.sort(key=lambda par: p_params.index(par))  # Make sure they are in the same order as p_params\n",
    "    model.name = model.__name__  # Easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the grids\n",
    "...in $\\bf \\mu$-space for each model, define the interpolation grids for later as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_mu_grid = {(model.name, mu_par): 32 for model in models for mu_par in model.mu_params}  # Default\n",
    "num_mu_grid['M_chirp_power_law_cutoff', 'Mchirp_max'] = 64  # Edit individual parameter numpoints\n",
    "num_mu_grid['gaussian_2ndlocked_chi_eff', 'std_chi_eff'] = 64\n",
    "for model in models:\n",
    "    if len(model.mu_params) == 1:  # Cheap ones\n",
    "        num_mu_grid[model.name, model.mu_params[0]] = 128\n",
    "\n",
    "mu_grid, mu_grid_1d = OrderedDict(), OrderedDict()\n",
    "for model in models:\n",
    "    for mu_par in model.mu_params:\n",
    "        mu_grid_1d[model.name, mu_par] = np.linspace(*model.mu_bounds[mu_par],\n",
    "                                                         num=num_mu_grid[model.name, mu_par])\n",
    "    mu_grid.update(OrderedDict(zip(\n",
    "        ((model.name, mu_par) for mu_par in model.mu_params),\n",
    "        np.meshgrid(*[mu_grid_1d[model.name, mu_par] for mu_par in model.mu_params], indexing='ij'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Interpolation grid in mu-space\n",
    "zoom = 4\n",
    "num_mu_igrid = {key: zoom*n for key, n in num_mu_grid.items()}\n",
    "\n",
    "mu_igrid, mu_igrid_1d = OrderedDict(), OrderedDict()\n",
    "mu_igrid_2d_params, mu_igrid_2d = OrderedDict(), OrderedDict()\n",
    "for model in models:\n",
    "    for mu_par in model.mu_params:\n",
    "        mu_igrid_1d[model.name, mu_par] = np.linspace(*model.mu_bounds[mu_par],\n",
    "                                                          num=num_mu_igrid[model.name, mu_par])\n",
    "    mu_igrid.update(OrderedDict(zip(\n",
    "        ((model.name, mu_par) for mu_par in model.mu_params),\n",
    "        np.meshgrid(*[mu_igrid_1d[model.name, mu_par] for mu_par in model.mu_params], indexing='ij'))))\n",
    "\n",
    "    # Will need subsets of 2 for plotting later\n",
    "    mu_igrid_2d_params[model.name] = [(x_par, y_par) for i, x_par in enumerate(model.mu_params) \n",
    "                                          for y_par in model.mu_params[i+1:]]\n",
    "    for xy_pars in mu_igrid_2d_params[model.name]:\n",
    "        mu_igrid_2d[model.name, xy_pars] = dict(zip(\n",
    "            xy_pars, np.meshgrid(*[mu_igrid_1d[model.name, mu_par] for mu_par in xy_pars], indexing='ij')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the data likelihood $P({\\rm data \\mid model}, \\mu)$\n",
    "$$\\begin{aligned}\n",
    "    P({\\rm data \\mid model}, \\mu) &\\propto \\prod_{i \\in \\rm\\{events\\}} \\int {\\rm d}p \\mathcal{L}_i(p) r(p, \\mu) \\\\\n",
    "    &= \\frac{\\prod_i \\int {\\rm d}p \\mathcal{L}_i(p) \\lambda(p, \\mu)} {\\left(\\int {\\rm d}p~\\lambda(p, \\mu) \\right)^{N_{\\rm events}}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "The $p$-integrals in the numerator are performed over each event's $p$-grid (where $\\mathcal{L}_i(p) \\neq 0$), the one in the denominator is over all $p$-space. We compute the denominator first:\n",
    "\n",
    "### Total detector-rate $\\overline\\lambda(\\mu)$\n",
    "\n",
    "If $R(\\mu, p)$ depends only in some of the $p$-parameters, we can speed up the integration. Let $p_1$ be the parameters on which $R$ depends and $p_2$ the ones on which is does not. The advantage is that we can do the $p_2$ integrals once and use it for all $\\mu$.\n",
    "The total rate is:\n",
    "\n",
    "$$\n",
    "  \\overline\\lambda(\\mu) = 4\\pi \\int {\\rm d}p\\,{\\rm d}D_L D_L^2 \\frac{1}{(1+z)^5} \n",
    "    R \\left( \\mathcal M_s = \\frac{\\mathcal M}{1+z}, q, \\chi_{\\rm eff},\\ldots \\,\\middle|\\, \\mu \\right) \n",
    "    P_{A>}\\left( \\frac{\\rho_{th}}{\\rho_0(p)}D_L \\right).\n",
    "$$\n",
    "\n",
    "We will change variables $u = D_L / D_{L, {\\rm max}}$, where $D_{L, {\\rm max}}$ is the horizon distance maximized over $p$.\n",
    "\n",
    "* If $R$ is independent of the chirp mass ($\\mathcal M \\in p_2$), we can ignore the source-frame mass dependence on $z$ and do \n",
    "$$\n",
    "  \\overline\\lambda(\\mu) = 4\\pi D_{L, \\rm max}^3\\int {\\rm d}p_1\\int_0^1 {\\rm d}u\\, u^2 \\frac{1}{(1+z)^5} \n",
    "    P_{A>}\\left( \\frac{\\rho_{th}}{\\rho_0(p)}D_L \\right)\n",
    "    \\int {\\rm d}p_2\n",
    "    R \\left(q, \\chi_{\\rm eff},\\ldots \\,\\middle|\\, \\mu \\right)\n",
    "$$ \n",
    "and compute the $p_2$ integral over a $p_1$-$u$ grid.\n",
    "\n",
    "* If $R$ depends on the chirp mass ($\\mathcal M \\in p_1$), we have to integrate over it for every $\\mu$. But here we can exploit the fact that the steepest sensitive-volume dependence is on the mass and treat $u$ as a $p_2$ parameter. Now we take $u = D_L / D_L(\\mathcal M, q=1, \\chi_{\\rm eff}=1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bad51091bc72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mredshifted_p1_igrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'M_chirp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredshifted_p1_igrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'M_chirp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mredshift\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mredshifted_p1_igrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Physical rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mtot_det_rate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mintegrand\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# If the rate is independent of mass, redshift doesn't affect it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mp1_igrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeshgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_igrid_1d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp1_params\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ij'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    p1_params = model.p_params\n",
    "    p2_params = [p_par for p_par in p_params if not p_par in p1_params]\n",
    "    \n",
    "    p2_indices = tuple(p_params.index(par) for par in p2_params)\n",
    "    D1, DL, integrand, redshift = {}, {}, {}, {}\n",
    "    \n",
    "    if 'M_chirp' in p1_params:\n",
    "        p1_igrid = OrderedDict(zip(p1_params + ['u'], \n",
    "                                   np.meshgrid(*[p_igrid_1d[par] for par in p1_params], p_grid_1d['u'], \n",
    "                                               indexing='ij')))\n",
    "        for PA in PSDs_approximants:\n",
    "            D1[PA] = np.amax(p_igrid[(*PA), 'SNR_1Mpc'],\n",
    "                             axis=p2_indices, keepdims=True)[...,np.newaxis] / SNR_thresh\n",
    "            redshift[PA] = z_of_DL(p1_igrid['u'] * D1[PA].sum(axis=p2_indices))\n",
    "            integrand[PA] = (\n",
    "                p1_igrid['u']**2 / (1 + redshift[PA])**5\n",
    "                * (P_gr_w(((SNR_thresh / p_igrid[(*PA), 'SNR_1Mpc'])[..., np.newaxis] * p_grid_1d['u']) * D1[PA])\n",
    "                   * D1[PA]**3).sum(axis=p2_indices))\n",
    "        # Compute, on the mu_grid, the detector rate averaged over the p_grid\n",
    "        tot_det_rate = {PA: [] for PA in PSDs_approximants}\n",
    "        for mu_values in np.array([mu_grid[model.name, mu_par].flatten() for mu_par in model.mu_params]).T:\n",
    "            for PA in PSDs_approximants:\n",
    "                redshifted_p1_igrid = deepcopy(p1_igrid)\n",
    "                redshifted_p1_igrid['M_chirp'] = redshifted_p1_igrid['M_chirp'] / (1 + redshift[PA])\n",
    "                R = model(**redshifted_p1_igrid, **dict(zip(model.mu_params, mu_values)))  # Physical rate\n",
    "                tot_det_rate[PA].append((R * integrand[PA]).sum())\n",
    "    else:  # If the rate is independent of mass, redshift doesn't affect it\n",
    "        p1_igrid = OrderedDict(zip(p1_params, np.meshgrid(*[p_igrid_1d[par] for par in p1_params], indexing='ij')))\n",
    "        for PA in PSDs_approximants:\n",
    "            DL[PA] = p_igrid[(*PA), 'SNR_1Mpc'] / SNR_thresh\n",
    "            redshift[PA] = z_of_DL(DL[PA][..., np.newaxis] * p_grid_1d['u'])\n",
    "            integrand[PA] = ((((1 + redshift[PA])**-5) \n",
    "                              * (p_grid_1d['u']**2 * P_gr_w(p_grid_1d['u']))).sum(axis=-1)\n",
    "                             * DL[PA]**3).sum(axis=p2_indices)\n",
    "\n",
    "        # Compute, on the mu_grid, the detector rate averaged over the p_grid\n",
    "        tot_det_rate = {PA: [] for PA in PSDs_approximants}\n",
    "        for mu_values in np.array([mu_grid[model.name, mu_par].flatten() for mu_par in model.mu_params]).T:\n",
    "            for PA in PSDs_approximants:\n",
    "                R = model(**p1_igrid, **dict(zip(model.mu_params, mu_values)))  # Physical rate\n",
    "                tot_det_rate[PA].append((R * integrand[PA]).sum())\n",
    "    \n",
    "    \n",
    "    for PA in PSDs_approximants:  # Reshape and store tot_det_rate\n",
    "        mu_grid[model.name, 'tot_det_rate', (*PA)] = np.reshape(\n",
    "            tot_det_rate[PA], mu_grid[model.name, model.mu_params[0]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mchirp_max': 15.0, 'alpha': -3.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(model.mu_params, mu_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $P({\\rm data \\mid model}, \\mu)$\n",
    "Now we compute the numerator terms $\\int {\\rm d}p \\mathcal{L}_i(p) \\lambda(p)$, and the total $P({\\rm data \\mid model}, \\mu)$ normalized to $\\int {\\rm d}\\mu P = 1$, doing the same trick with $p_1, p_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    numerator = {PCA: np.ones_like(mu_grid[model.name, model.mu_params[0]])\n",
    "                 for PCA in PSDs_coherences_approximants}\n",
    "    p1_params = model.p_params\n",
    "    p2_params = [p_par for p_par in p_params if not p_par in p1_params]\n",
    "    p2_indices = tuple(p_params.index(par) for par in p2_params)\n",
    "    if 'M_chirp' in p1_params:\n",
    "        for event in events:\n",
    "            p1_grid = OrderedDict(zip(p1_params + ['u'], \n",
    "                                      np.meshgrid(*[p_grid_1d[event][par] for par in p1_params], p_grid_1d['u'],\n",
    "                                                             indexing='ij')))\n",
    "            D1, integrand, redshift = {}, {}, {}\n",
    "            for PCA in PSDs_coherences_approximants:\n",
    "                CA = PCA[1:]\n",
    "                PA = PCA[::2]\n",
    "                D1[PA] = np.amax(p_grid[event]['SNR_1Mpc_' + '_'.join(PA)],\n",
    "                                 axis=p2_indices,keepdims=True)[...,np.newaxis] / SNR_thresh\n",
    "                redshift[PA] = z_of_DL(p1_grid['u'] * D1[PA].sum(axis=p2_indices))\n",
    "                integrand[PCA] = (\n",
    "                    p1_grid['u']**2 / (1 + redshift[PA])**5\n",
    "                    * (P_gr_w(((SNR_thresh / p_grid[event]['SNR_1Mpc_' + '_'.join(PA)])[..., np.newaxis] \n",
    "                               * p_grid_1d['u']) * D1[PA])\n",
    "                       * p_grid[event]['likelihood_' + '_'.join(CA)][..., np.newaxis] \n",
    "                       * D1[PA]**3).sum(axis=p2_indices))\n",
    "\n",
    "            numerator_i = {PCA: [] for PCA in PSDs_coherences_approximants}\n",
    "            for mu_values in np.array([mu_grid[model.name, mu_par].flatten() for mu_par in model.mu_params]).T:\n",
    "                for PCA in PSDs_coherences_approximants:\n",
    "                    PA = PCA[::2]\n",
    "                    redshifted_p1_grid = deepcopy(p1_grid)\n",
    "                    redshifted_p1_grid['M_chirp'] = redshifted_p1_grid['M_chirp'] / (1 + redshift[PA])\n",
    "                    R = model(**redshifted_p1_grid, **dict(zip(model.mu_params, mu_values)))  # Physical rate\n",
    "                    numerator_i[PCA].append((R * integrand[PCA]).sum())\n",
    "            for PCA in PSDs_coherences_approximants:\n",
    "                numerator[PCA] *= np.reshape(\n",
    "                    numerator_i[PCA], mu_grid[model.name, model.mu_params[0]].shape)\n",
    "    else:\n",
    "        for event in events:\n",
    "            p1_grid = OrderedDict(zip(p1_params, np.meshgrid(*[p_grid_1d[event][par] for par in p1_params],\n",
    "                                                             indexing='ij')))\n",
    "            DL, integrand, redshift = {}, {}, {}\n",
    "            for PCA in PSDs_coherences_approximants:\n",
    "                CA = PCA[1:]\n",
    "                PA = PCA[::2]\n",
    "                DL[PA] = p_grid[event]['SNR_1Mpc_' + '_'.join(PA)] / SNR_thresh\n",
    "                redshift[PA] = z_of_DL(DL[PA][..., np.newaxis] * p_grid_1d['u'])\n",
    "                integrand[PCA] = ((((1 + redshift[PA])**-5) \n",
    "                                   * (p_grid_1d['u']**2 * P_gr_w(p_grid_1d['u']))).sum(axis=-1)\n",
    "                                  * (p_grid[event]['likelihood_' + '_'.join(CA)] * DL[PA]**3)\n",
    "                                 ).sum(axis=p2_indices)\n",
    "\n",
    "            numerator_i = {PCA: [] for PCA in PSDs_coherences_approximants}\n",
    "            for mu_values in np.array([mu_grid[model.name, mu_par].flatten() for mu_par in model.mu_params]).T:\n",
    "                for PCA in PSDs_coherences_approximants:\n",
    "                    R = model(**p1_grid, **dict(zip(model.mu_params, mu_values)))  # Physical rate\n",
    "                    numerator_i[PCA].append((R * integrand[PCA]).sum())\n",
    "            for PCA in PSDs_coherences_approximants:\n",
    "                numerator[PCA] *= np.reshape(\n",
    "                    numerator_i[PCA], mu_grid[model.name, model.mu_params[0]].shape)\n",
    "                \n",
    "    for PCA in PSDs_coherences_approximants:\n",
    "        PA = PCA[::2]\n",
    "        P = (numerator[PCA] / mu_grid[model.name, 'tot_det_rate', (*PA)]**len(events))\n",
    "        mu_grid[model.name, 'P', (*PCA)] = P / P.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate \n",
    "... $P({\\rm data \\mid model}, \\mu)$ on a refined grid in $\\mu$-space, `mu_igrid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    for PCA in PSDs_coherences_approximants:\n",
    "        P = ndimage.zoom(np.reshape(\n",
    "            mu_grid[model.name, 'P', (*PCA)],\n",
    "            tuple(num_mu_grid[model.name, mu_par] for mu_par in model.mu_params)), zoom, order=1)\n",
    "        mu_igrid[model.name, 'P', (*PCA)] = P / P.sum()  # Normalize again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginalize\n",
    "\n",
    "... over some $\\mu$ variables, on the interpolation grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    # Compute 1d marginalized likelihoods P(data|model):\n",
    "    for i, x_par in enumerate(model.mu_params):\n",
    "        js = tuple(j for j in range(len(model.mu_params)) if j != i) # Marginalize over these axes\n",
    "        for PCA in PSDs_coherences_approximants:\n",
    "            mu_igrid_1d[model.name, 'P', x_par, (*PCA)] \\\n",
    "                = mu_igrid[model.name, 'P', (*PCA)].sum(axis=js)\n",
    "    # Compute 2d marginalized likelihoods\n",
    "    for xy_pars in mu_igrid_2d_params[model.name]:\n",
    "        for PCA in PSDs_coherences_approximants:\n",
    "            js = tuple(j for j, z in enumerate(model.mu_params) if not z in xy_pars) # Marginalize over these\n",
    "            mu_igrid_2d[model.name, xy_pars]['P', (*PCA)] = mu_igrid[model.name, 'P', (*PCA)].sum(axis=js)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find $\\mu$-parameter estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "median, bounds_estimate, err_estimate = {}, {}, {}\n",
    "for model in models:\n",
    "    for PCA in PSDs_coherences_approximants:\n",
    "        for mu_par in model.mu_params:\n",
    "            cumulative_P = UnivariateSpline(\n",
    "                mu_igrid_1d[model.name, mu_par],\n",
    "                np.cumsum(mu_igrid_1d[model.name, 'P', mu_par, (*PCA)]) - .5, s=0)\n",
    "            median[model.name, (*PCA), mu_par] = cumulative_P.roots()[0]\n",
    "            P_level = brentq(lambda level, P: P[P > level].sum()-.9, 0, 1, \n",
    "                             args=(mu_igrid_1d[model.name, 'P', mu_par, (*PCA)]))\n",
    "            P_spline = UnivariateSpline(mu_igrid_1d[model.name, mu_par], \n",
    "                                        mu_igrid_1d[model.name, 'P', mu_par, (*PCA)] - P_level, s=0)\n",
    "            roots = P_spline.roots()\n",
    "            bounds_estimate[model.name, (*PCA), mu_par] = np.array([roots[0], roots[-1]])\n",
    "            if mu_igrid_1d[model.name, 'P', mu_par, (*PCA)][0] > P_level:\n",
    "                bounds_estimate[model.name, (*PCA), mu_par][0] = mu_igrid_1d[model.name, mu_par][0]\n",
    "            if mu_igrid_1d[model.name, 'P', mu_par, (*PCA)][-1] > P_level:\n",
    "                bounds_estimate[model.name, (*PCA), mu_par][1] = mu_igrid_1d[model.name, mu_par][-1]\n",
    "            err_estimate[model.name, (*PCA), mu_par] = abs(median[model.name, (*PCA), mu_par]\n",
    "                                                           - bounds_estimate[model.name, (*PCA), mu_par])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find % confidence contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fractions = [.5, .9]  # Probability enclosed by contours\n",
    "fractions = sorted(fractions, reverse=True)  # So that the P levels are increasing\n",
    "levels = {}\n",
    "for model in models:\n",
    "    for PCA in PSDs_coherences_approximants:\n",
    "        for xy_pars in mu_igrid_2d_params[model.name]:\n",
    "            levels[model.name, (*PCA), xy_pars] = [brentq(\n",
    "                lambda level, P: P[P > level].sum()-fraction, 0, 1,\n",
    "                args=(mu_igrid_2d[model.name, xy_pars]['P', (*PCA)])) for fraction in fractions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def latex_val_err(v, e):\n",
    "    '''Gets a value and its uncertainty, returns a latex string\n",
    "    $v^{+ep}_{-em}$ with the significant figures given by the uncertainties.\n",
    "    v: float\n",
    "    e: [err_m, err_p]\n",
    "    '''\n",
    "    n_digits = max(0, *[int(np.ceil(-np.log10(e_))) for e_ in e])\n",
    "    r = lambda a, n: round(a, n) if n > 0 else int(round(a))\n",
    "    rv = r(v, n_digits)\n",
    "    return '${}_{{-{}}}^{{+{}}}$'.format(rv, *[r(e_ - v + rv, n_digits) for e_ in e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.clf();\n",
    "for model in models:\n",
    "    print(model.name)\n",
    "    n_cols = n_rows = len(model.mu_params)\n",
    "    for PCA in PSDs_coherences_approximants:\n",
    "        fig, ax = plt.subplots(n_rows, n_cols, figsize=(2.7 * n_cols, 2.7 * n_rows + .5));\n",
    "        if n_cols == 1: ax = [[ax]]\n",
    "        plt.suptitle('{}'.format(model.latex), size='large')\n",
    "\n",
    "        # Plot 2D likelihoods (off-diagonal in the plot grid)\n",
    "        for row, y_par in list(enumerate(model.mu_params))[1:]:\n",
    "            for col, x_par in list(enumerate(model.mu_params))[:row]:\n",
    "                plt.sca(ax[row][col])\n",
    "                xy_P = mu_igrid_2d[model.name, (x_par, y_par)]['P', (*PCA)].T\n",
    "\n",
    "                plt.imshow(xy_P, extent=[*model.mu_bounds[x_par], *model.mu_bounds[y_par]],\n",
    "                           cmap='Oranges', origin='lower', aspect='auto', alpha=.8)\n",
    "\n",
    "                contours = plt.contour(mu_igrid_1d[model.name, x_par],\n",
    "                                       mu_igrid_1d[model.name, y_par],\n",
    "                                       xy_P,\n",
    "                                       levels=levels[model.name, (*PCA), (x_par, y_par)],\n",
    "                                       colors=['tab:orange', 'tab:brown'])\n",
    "                for i in range(len(fractions)):\n",
    "                    contours.collections[i].set_label('{:.0f}% c.l.'.format(100*fractions[i]))\n",
    "\n",
    "        # Plot 1D likelihoods (diagonal)\n",
    "        for i, mu_par in enumerate(model.mu_params):\n",
    "            plt.sca(ax[i][i])\n",
    "            for val in [median[model.name, (*PCA), mu_par], *bounds_estimate[model.name, (*PCA), mu_par]]:\n",
    "                plt.plot([val]*2, [0, np.interp(val, mu_igrid_1d[model.name, mu_par],\n",
    "                                                mu_igrid_1d[model.name, 'P', mu_par, (*PCA)])],\n",
    "                          'C0', alpha=.5)\n",
    "            span = np.linspace(*bounds_estimate[model.name, (*PCA), mu_par])\n",
    "            plt.fill_between(span, 0, np.interp(span, mu_igrid_1d[model.name, mu_par],\n",
    "                                                mu_igrid_1d[model.name, 'P', mu_par, (*PCA)]), alpha=.1)\n",
    "            \n",
    "            plt.title('{}$=${} {}'.format(\n",
    "                latex[mu_par],\n",
    "                latex_val_err(median[model.name, (*PCA), mu_par],\n",
    "                              err_estimate[model.name, (*PCA), mu_par]),\n",
    "                unit[mu_par].replace('(', '').replace(')','')))\n",
    "\n",
    "            plt.plot(mu_igrid_1d[model.name, mu_par],\n",
    "                     mu_igrid_1d[model.name, 'P', mu_par, (*PCA)], \n",
    "                     label=r'$P(\\{d_i\\} \\mid \\mathbf{\\mu})$')\n",
    "\n",
    "        # Embellish\n",
    "        for col, x_par in enumerate(model.mu_params):\n",
    "            ax[n_rows-1][col].set_xlabel(latex[x_par] + unit[x_par], size='large')\n",
    "            if len(model.mu_params) > 1:\n",
    "                plt.setp(ax[n_rows-1][col].get_xticklabels(), rotation=45)\n",
    "            for row in range(n_rows-1):\n",
    "                ax[row][col].tick_params(labelbottom='off')\n",
    "        for row, y_par in list(enumerate(model.mu_params))[1:]:\n",
    "            ax[row][0].set_ylabel(latex[y_par] + unit[y_par], size='large')\n",
    "            if len(model.mu_params) > 1:\n",
    "                plt.setp(ax[row][0].get_yticklabels(), rotation=45)\n",
    "            for col in range(1, n_cols):\n",
    "                ax[row][col].tick_params(labelleft='off')\n",
    "        for row in range(n_rows-1):\n",
    "            for col in range(row+1, n_cols):\n",
    "                ax[row][col].axis('off')\n",
    "        for col in range(n_cols):\n",
    "            for row in range(1, n_rows):\n",
    "                ax[0][col].get_shared_x_axes().join(ax[0][col], ax[row][col])\n",
    "                ax[row][col].autoscale()\n",
    "        for row in range(n_rows):\n",
    "            for col in range(1, row):\n",
    "                ax[row][0].get_shared_y_axes().join(ax[row][0], ax[row][col])\n",
    "                ax[row][col].autoscale()\n",
    "        for i in range(len(model.mu_params)):\n",
    "            ax[i][i].tick_params(axis='y', left='off', labelleft='off')\n",
    "            ax[i][i].set_ylim(ymin=0)\n",
    "        ax[-1][-1].set_xlim(model.mu_bounds[model.mu_params[-1]])\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96]);\n",
    "        plt.subplots_adjust(hspace=0.04, wspace=0.04);\n",
    "\n",
    "        handles_1d, labels_1d = ax[0][0].get_legend_handles_labels()\n",
    "        if len(model.mu_params) > 1:\n",
    "            handles_2d, labels_2d = ax[1][0].get_legend_handles_labels()\n",
    "            pos = ax[0][1].get_position()\n",
    "            fig.legend(handles_1d + handles_2d, labels_1d + labels_2d, \n",
    "                       loc='upper left', bbox_to_anchor=(pos.x0, pos.y1), frameon=False)\n",
    "        else:\n",
    "            ax[0][0].legend(handles_1d, [r'$P(\\{d_i\\} \\mid $'+latex[model.mu_params[0]]+'$)$'],\n",
    "                            loc='best', frameon=False)\n",
    "\n",
    "        plt.savefig('figures/{}_{}.pdf'.format(model.name, '_'.join(PCA)), bbox_inches='tight')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
