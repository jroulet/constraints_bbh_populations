{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate population parameters\n",
    "\n",
    "...by computing the likelihood on a grid.\n",
    "\n",
    "Notation:\n",
    "* $p = \\{\\mathcal{M}, q, \\chi_{\\rm eff}, \\ldots \\}$ are the physical parameters of the merger, `p_params`.\n",
    "* $\\mu$ are the population-model parameters, `mu_params`.\n",
    "* $R(p, \\mu)$ are modeled universe-rates, so that $R(p, \\mu){\\rm d}p$ has units of $(VT)^{-1}$.\n",
    "* $\\lambda(p, \\mu) = V(p) R(p, \\mu)$ are the detector-rates, where $V(p)$ is the detector network sensitive volume averaged over angles (in practice, $\\propto {\\rm SNR_{1Mpc}^3}$).\n",
    "* $\\overline\\lambda(\\mu) = \\int {\\rm d}p V(p) R(p, \\mu)$ is the total detector-rate.\n",
    "* $r(p, \\mu) = \\lambda(p, \\mu) / \\overline\\lambda(\\mu)$ are the *relative* detector-rates, normalized so that $\\int r(p, \\mu){\\rm d}p = 1$.\n",
    "* $\\mathcal{L}_i(p)$ is the likelihood that the $i$-th GW event has parameters $p$, normalized to $\\int \\mathcal{L}_i(p){\\rm d}p = 1$.\n",
    "\n",
    "$$P({\\rm data \\mid model}, \\mu) \\propto \\prod_{i \\in \\rm \\{events\\}} \\int {\\rm d}p ~ r(p, \\mu) \\mathcal{L}_i(p)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndimage\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.optimize import brentq as brentq\n",
    "from scipy.special import erf\n",
    "from numpy import sqrt, pi\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = ['GW150914',\n",
    "          'GW151226',\n",
    "          'LVT151012',\n",
    "          'GW170104',\n",
    "          'GW170608',\n",
    "          'GW170814'\n",
    "         ]\n",
    "\n",
    "approximants = [#'IMRPhenomD',\n",
    "                'SEOBNRv4_ROM'\n",
    "               ]\n",
    "PSDs = [#'ASD_H1_GW150914', \n",
    "        #'ASD_zdhp',\n",
    "        'ASD_average',\n",
    "       ]\n",
    "coherences = ['coherent',\n",
    "              #'incoherent',\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PSDs_coherences_approximants = [(PSD, coherence, approximant) for PSD in PSDs \n",
    "                                                              for coherence in coherences\n",
    "                                                              for approximant in approximants]\n",
    "\n",
    "coherences_approximants = [(coherence, approximant) for coherence in coherences \n",
    "                                                    for approximant in approximants]\n",
    "\n",
    "PSDs_approximants = [(psd, approximant) for psd in PSDs\n",
    "                     for approximant in approximants]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "* Grid in $p$-space for each event $i$, including $\\log \\mathcal{L}_i(p)$  \n",
    "Compute the likelihood, normalize to $\\int \\mathcal{L}_i(p){\\rm d}p = 1$.\n",
    "\n",
    "* Sensitive volume $V(p) \\propto {\\rm SNR}_{1 \\rm Mpc}^3$ on a full $p$-grid  \n",
    "Interpolate $V(p)$ on the $p$-space grids of each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_grid = {}\n",
    "for event in events:\n",
    "    p_grid[event] = pd.read_csv(event + '/parameter_grid', sep='\\s+')\n",
    "    for CA in coherences_approximants:\n",
    "        L = np.exp(p_grid[event]['logL_' + '_'.join(CA)])\n",
    "        L /= L.sum()\n",
    "        if event == 'LVT151012':\n",
    "            L = .87*L + .13  # Marginalize over the possibility that LVT is noise\n",
    "        p_grid[event]['likelihood_' + '_'.join(CA)] = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load SNR_1Mpc on full p-grid\n",
    "p_params = ['M_chirp', 'q', 'chi_eff']\n",
    "p_grid['full'] = pd.read_csv('../2-estimate_horizon/all_par_space/parameter_grid', sep='\\s+')\n",
    "metadata = pd.read_csv('../2-estimate_horizon/all_par_space/grid_metadata', sep='\\s+')\n",
    "p_grid_1d = {par: np.sort(list(set(p_grid['full'][par].values))) for par in p_params}  # Of the full p-grid\n",
    "\n",
    "assert all(p_par in grid for p_par in p_params for grid in p_grid.values())\n",
    "assert all(metadata.columns == p_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Interpolate over each event's grid, and also refine the full p_grid => p_igrid\n",
    "min_val, max_val = ({p_par: f(p_grid['full'][p_par]) for p_par in p_params} for f in (min, max))\n",
    "zoom = 6\n",
    "p_igrid_1d = {p_par: np.linspace(min_val[p_par], max_val[p_par], zoom * metadata[p_par][0]) for p_par in p_params}\n",
    "p_igrid = OrderedDict(zip(p_params, np.meshgrid(*[p_igrid_1d[p_par] for p_par in p_params], indexing='ij')))\n",
    "for PA in PSDs_approximants:\n",
    "    V = p_grid['full']['SNR_1Mpc_' + '_'.join(PA)].values.reshape(metadata.values[0])**3\n",
    "    for event in events:\n",
    "        p_grid[event]['V_' + '_'.join(PA)] = ndimage.map_coordinates(\n",
    "            V, [(p_grid[event][p_par] - min_val[p_par]) * (\n",
    "                (metadata[p_par][0]-1) / (max_val[p_par]-min_val[p_par])) for p_par in p_params])\n",
    "    p_igrid[(*PA), 'V'] = ndimage.zoom(V, zoom, order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model physical rates\n",
    "\n",
    "Define some rate models, that return the physical rate $R(p, \\mu)$. The only important information is the ratio $R(p_1, \\mu) / R(p_2,\\mu)$ for the same $\\mu$. Normalization is otherwise arbitrary.\n",
    "\n",
    "### Models on spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_chi_eff(chi_eff, alpha, **kwargs):\n",
    "    return np.exp(alpha * chi_eff)\n",
    "simple_chi_eff.latex = r'$R(\\chi_{\\rm eff}) \\propto e^{\\alpha \\chi_{\\rm eff}}$'\n",
    "simple_chi_eff.p_params = ['chi_eff']\n",
    "simple_chi_eff.mu_params = ['alpha']\n",
    "simple_chi_eff.mu_bounds = {'alpha': [-5, 5]}\n",
    "\n",
    "def exponential_chi_eff(chi_eff, avg_chi_eff, **kwargs):\n",
    "    abs_avg = abs(avg_chi_eff)\n",
    "    alpha = (np.sign(avg_chi_eff) \n",
    "             * brentq(lambda a: 1/np.tanh(a) - 1/a - abs_avg, 3*abs_avg, 3 * (1/(1-abs_avg) - 1)))\n",
    "    assert(np.sign(avg_chi_eff) == np.sign(alpha))\n",
    "    return np.exp(alpha * chi_eff)\n",
    "exponential_chi_eff.latex = r'$R(\\chi_{\\rm eff}) \\propto e^{\\alpha \\chi_{\\rm eff}}$'\n",
    "exponential_chi_eff.p_params = ['chi_eff']\n",
    "exponential_chi_eff.mu_params = ['avg_chi_eff']\n",
    "exponential_chi_eff.mu_bounds = {'avg_chi_eff': [-.99, .99]}\n",
    "\n",
    "def both_spinning(q, chi_eff, alpha1, alpha2, **kwargs):\n",
    "    # A rustic but fast piecewise function:\n",
    "    cond1 = 1 + chi_eff + q*chi_eff <= q\n",
    "    cond2 = q + chi_eff + q*chi_eff > 1\n",
    "    cond3 = ~ (cond1 | cond2)  # Neither\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        R = np.divide( \n",
    "            cond1 * (np.exp(-alpha1 - alpha2)*(np.exp((1 + q)*alpha1*(1 + chi_eff))\n",
    "                     - np.exp(((1 + q)*alpha2*(1 + chi_eff))/q))*(1 + q))\n",
    "            + cond2 * (np.exp(-alpha1 - alpha2) * (-np.exp(2*alpha2 + alpha1*(1 + q*(-1 + chi_eff) + chi_eff))\n",
    "                       +np.exp(2*alpha1 + (alpha2*(-1 + q + chi_eff + q*chi_eff))/q))*(1+q))\n",
    "            + cond3 * (2*np.exp((1 + q)*alpha1*chi_eff)*(1 + q)*np.sinh(q*alpha1 - alpha2))\n",
    "            , (q*alpha1 - alpha2))\n",
    "    R[np.isinf(R) | np.isnan(R)] = 0  # Exclude nans and infs\n",
    "    return R\n",
    "both_spinning.latex = r'$R(\\chi_i) \\propto e^{\\alpha_i \\chi_i}$'\n",
    "both_spinning.p_params = ['q', 'chi_eff']\n",
    "both_spinning.mu_params = ['alpha1', 'alpha2']\n",
    "both_spinning.mu_bounds = {'alpha1': [-5, 5],\n",
    "                           'alpha2': [-5, 5]}\n",
    "\n",
    "def zero_mean_gaussian_chi_eff(chi_eff, std_chi_eff, **kwargs):\n",
    "    return np.exp(-.5 * chi_eff**2 / std_chi_eff**2)\n",
    "zero_mean_gaussian_chi_eff.latex = (r'$R(\\chi_{\\rm eff}) \\propto '\n",
    "                                    r'\\exp(-\\chi_{\\rm eff}^2 / 2 \\sigma_{\\chi_{\\rm eff}}^2)}$')\n",
    "zero_mean_gaussian_chi_eff.p_params = ['chi_eff']\n",
    "zero_mean_gaussian_chi_eff.mu_params = ['std_chi_eff']\n",
    "zero_mean_gaussian_chi_eff.mu_bounds = {'std_chi_eff': [5e-3, .5]}\n",
    "\n",
    "def gaussian_chi_eff(chi_eff, mean_chi_eff, std_chi_eff, **kwargs):\n",
    "    return np.exp(-(chi_eff - mean_chi_eff)**2 / std_chi_eff**2 / 2)\n",
    "gaussian_chi_eff.latex = (r'$R(\\chi_{\\rm eff}) \\propto \\exp \\left('\n",
    "    r'-(\\chi_{\\rm eff}-\\overline{\\chi}_{\\rm eff})^2 / 2 \\sigma_{\\chi_{\\rm eff}}^2 \\right)$')\n",
    "gaussian_chi_eff.p_params = ['chi_eff']\n",
    "gaussian_chi_eff.mu_params = ['mean_chi_eff', 'std_chi_eff']\n",
    "gaussian_chi_eff.mu_bounds = {'mean_chi_eff': [-.3, .3],\n",
    "                              'std_chi_eff': [5e-3, .45]}\n",
    "\n",
    "def gaussian_2ndlocked_chi_eff(q, chi_eff, std_chi_eff, f, **kwargs):\n",
    "    '''A zero-mean gaussian population * (1-f), plus a secondary-locked population * f.'''\n",
    "    return ((1-f) * np.exp(-chi_eff**2 / std_chi_eff**2 / 2)\n",
    "            + f * sqrt(1+q**2) * np.exp(-(chi_eff-q/(1+q))**2 * (1+q**2) / std_chi_eff**2 / 2))\n",
    "gaussian_2ndlocked_chi_eff.latex = (r'$R(\\chi_{\\rm eff}, q) \\propto'\n",
    "    r'(1-f) G(\\chi_{\\rm eff}, \\sigma_{\\chi_{\\rm eff}})'\n",
    "    r'+ f \\, G(\\chi_{\\rm eff} - \\frac{q}{1+q}, \\frac{\\sigma_{\\chi_{\\rm eff}}}{\\sqrt{1+q^2}})$')\n",
    "gaussian_2ndlocked_chi_eff.p_params = ['q', 'chi_eff']\n",
    "gaussian_2ndlocked_chi_eff.mu_params = ['f', 'std_chi_eff']\n",
    "gaussian_2ndlocked_chi_eff.mu_bounds = {'f': [0, 1],\n",
    "                                        'std_chi_eff': [1e-2, .4]}\n",
    "\n",
    "def gaussian_2ndlocked_chi_eff_old(q, chi_eff, std_chi_eff, f, **kwargs):\n",
    "    '''Like above but assuming the primary is non-spinning for the locked case.'''\n",
    "    width_chi_eff = 5e-2  # The width of the 'delta function'\n",
    "    return ((1-f) * np.exp(-chi_eff**2 / std_chi_eff**2 / 2) \n",
    "                  / (sqrt(2*pi) * std_chi_eff * erf(1/(sqrt(2)*std_chi_eff)))\n",
    "            + f * (np.abs(chi_eff - q/(1+q)) < width_chi_eff)) / width_chi_eff\n",
    "gaussian_2ndlocked_chi_eff_old.latex = (r'$R(\\chi_{\\rm eff}, q) \\propto'\n",
    "    r'(1-f) G (\\chi_{\\rm eff}, \\sigma_{\\chi_{\\rm eff}})'\n",
    "    r'+ f \\, \\delta(\\chi_{\\rm eff} - \\frac{q}{1+q})$')\n",
    "gaussian_2ndlocked_chi_eff_old.p_params = ['q', 'chi_eff']\n",
    "gaussian_2ndlocked_chi_eff_old.mu_params = ['f', 'std_chi_eff']\n",
    "gaussian_2ndlocked_chi_eff_old.mu_bounds = {'f': [0, 1],\n",
    "                                        'std_chi_eff': [1e-2, .4]}\n",
    "\n",
    "def a_bar_random_angle(q, chi_eff, a_bar, **kwargs):\n",
    "    a = np.maximum(((1+q)*chi_eff - a_bar)/q, -a_bar)\n",
    "    b = np.minimum(((1+q)*chi_eff + a_bar)/q, a_bar)\n",
    "    return (1+q)*(b-a) * (b-a > 0)\n",
    "a_bar_random_angle.latex = r'$R(\\chi_{1, 2}) = U(-\\overline{a}, \\overline{a})$'\n",
    "a_bar_random_angle.p_params = ['q', 'chi_eff']\n",
    "a_bar_random_angle.mu_params = ['a_bar']\n",
    "a_bar_random_angle.mu_bounds = {'a_bar': [1e-2, 1]}\n",
    "\n",
    "def a_bar_mu_avg(q, chi_eff, a_bar, mu_avg, **kwargs):\n",
    "    mu_min = 2*mu_avg - 1\n",
    "    a = np.maximum(((1+q)*chi_eff - a_bar)/q, mu_min*a_bar)\n",
    "    b = np.minimum(((1+q)*chi_eff - mu_min*a_bar)/q, a_bar)\n",
    "    return (1+q)*(b-a) * (b-a > 0)\n",
    "a_bar_mu_avg.latex = r'$R(\\chi_{1, 2}) = U(\\mu_{\\rm min}\\overline{a}, \\overline{a})$'\n",
    "a_bar_mu_avg.p_params = ['q', 'chi_eff']\n",
    "a_bar_mu_avg.mu_params = ['a_bar', 'mu_avg']\n",
    "a_bar_mu_avg.mu_bounds = {'a_bar': [1e-2, .85],\n",
    "                          'mu_avg': [0, .85]}\n",
    "\n",
    "def a_bar_equal_angles(chi_eff, a_bar, **kwargs):\n",
    "    return np.abs(chi_eff) < a_bar\n",
    "a_bar_equal_angles.latex = r'$R(\\chi_{\\rm eff}) = U(-\\overline{a}, \\overline{a})$'\n",
    "a_bar_equal_angles.p_params = ['chi_eff']\n",
    "a_bar_equal_angles.mu_params = ['a_bar']\n",
    "a_bar_equal_angles.mu_bounds = {'a_bar': [1e-2, .85]}\n",
    "\n",
    "def a_bar_mu_avg_equal_angles(chi_eff, a_bar, mu_avg, **kwargs):\n",
    "    mu_min = 2*mu_avg - 1\n",
    "    return (chi_eff > mu_min*a_bar) & (chi_eff < a_bar)\n",
    "a_bar_mu_avg_equal_angles.latex = r'$R(\\chi_{\\rm eff}) = U(\\mu_{\\rm min}\\overline{a}, \\overline{a})$'\n",
    "a_bar_mu_avg_equal_angles.p_params = ['chi_eff']\n",
    "a_bar_mu_avg_equal_angles.mu_params = ['a_bar', 'mu_avg']\n",
    "a_bar_mu_avg_equal_angles.mu_bounds = {'a_bar': [1e-2, .85],\n",
    "                          'mu_avg': [0, .85]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models on mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def M_chirp_power_law_cutoff(M_chirp, alpha, Mchirp_max, Mchirp_min=5, **kwargs):\n",
    "    if alpha == 0: alpha = 1e-10  # 0 is problematic\n",
    "    return M_chirp**-alpha * (5 < M_chirp) * (M_chirp < Mchirp_max)\n",
    "M_chirp_power_law_cutoff.latex = (r'$R(\\mathcal{{M}}) \\propto \\mathcal{{M}}^{{-\\alpha}}$, '\n",
    "                                  r'$\\mathcal{{M}} \\in (5 M_\\odot, \\mathcal{{M}}_{{\\rm max}})$')\n",
    "M_chirp_power_law_cutoff.p_params = ['M_chirp']\n",
    "M_chirp_power_law_cutoff.mu_params = ['alpha', 'Mchirp_max']\n",
    "M_chirp_power_law_cutoff.mu_bounds = {'alpha': [-3, 5],\n",
    "                                      'Mchirp_max': [25, 100]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models on mass ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_power_law(q, q_avg, **kwargs):\n",
    "    return q ** ((2*q_avg-1) / (1-q_avg))\n",
    "q_power_law.latex = r'$R(q) \\propto q^\\alpha$'\n",
    "q_power_law.p_params = ['q']\n",
    "q_power_law.mu_params = ['q_avg']\n",
    "q_power_law.mu_bounds = {'q_avg': [0, .99]}\n",
    "\n",
    "def q_half_gaussian(q, std_q, **kwargs):\n",
    "    return np.exp(-(q-1)**2 /2 /std_q**2)\n",
    "q_half_gaussian.latex = r'$R(q) \\propto \\exp(-(q-1)^2/2\\sigma_q^2)$'\n",
    "q_half_gaussian.p_params = ['q']\n",
    "q_half_gaussian.mu_params = ['std_q']\n",
    "q_half_gaussian.mu_bounds = {'std_q': [1e-2, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latex = {'M_chirp': r'$\\mathcal{{M}}$',\n",
    "         'q': r'$q$',\n",
    "         'chi_eff': r'$\\chi_{{\\rm eff}}$',\n",
    "         'alpha': r'$\\alpha$',\n",
    "         'beta': r'$\\beta$',\n",
    "         'alpha1': r'$\\alpha_1$',\n",
    "         'alpha2': r'$\\alpha_2$',\n",
    "         'Mchirp_max': r'$\\mathcal{{M}}_{{\\rm max}}$',\n",
    "         'mean_chi_eff': r'$\\overline{{\\chi}}_{{\\rm eff}}$',\n",
    "         'std_chi_eff': r'$\\sigma_{{\\chi_{{\\rm eff}}}}$',\n",
    "         'f': '$f$',\n",
    "         'q_avg': r'$\\langle q \\rangle$',\n",
    "         'avg_chi_eff': r'$\\langle \\chi_{{\\rm eff}} \\rangle$',\n",
    "         'std_q': r'$\\sigma_q$',\n",
    "         'a_bar': r'$\\overline{{a}}$',\n",
    "         'mu_avg': r'$\\overline{{\\mu}}$',\n",
    "        }\n",
    "unit = {par: r' (${\\rm M}_\\odot$)' if par in ['M_chirp', 'Mchirp_max'] else '' for par in latex}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [#simple_chi_eff,\n",
    "          #exponential_chi_eff,\n",
    "          #both_spinning,\n",
    "          #M_chirp_power_law_cutoff,\n",
    "          #zero_mean_gaussian_chi_eff,\n",
    "          #gaussian_chi_eff,\n",
    "          #gaussian_2ndlocked_chi_eff,\n",
    "          #gaussian_2ndlocked_chi_eff_old,\n",
    "          #q_power_law,\n",
    "          #q_half_gaussian,\n",
    "          a_bar_mu_avg,\n",
    "          #a_bar_random_angle,\n",
    "          #a_bar_equal_angles,\n",
    "          a_bar_mu_avg_equal_angles,\n",
    "         ]\n",
    "assert all(par in p_params for model in models for par in model.p_params)\n",
    "for model in models:\n",
    "    model.p_params.sort(key=lambda par: p_params.index(par))  # Make sure they are in the same order as p_params\n",
    "    model.name = model.__name__  # Easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the grids\n",
    "...in $\\bf \\mu$-space for each model, define the interpolation grids for later as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_mu_grid = {(model.name, mu_par): 32 for model in models for mu_par in model.mu_params}  # Default\n",
    "num_mu_grid['M_chirp_power_law_cutoff', 'Mchirp_max'] = 64  # Edit individual parameter numpoints\n",
    "for model in models:\n",
    "    if len(model.mu_params) == 1:  # Cheap ones\n",
    "        num_mu_grid[model.name, model.mu_params[0]] = 64\n",
    "\n",
    "mu_grid, mu_grid_1d = OrderedDict(), OrderedDict()\n",
    "for model in models:\n",
    "    for mu_par in model.mu_params:\n",
    "        mu_grid_1d[model.name, mu_par] = np.linspace(*model.mu_bounds[mu_par],\n",
    "                                                         num=num_mu_grid[model.name, mu_par])\n",
    "    mu_grid.update(OrderedDict(zip(\n",
    "        ((model.name, mu_par) for mu_par in model.mu_params),\n",
    "        np.meshgrid(*[mu_grid_1d[model.name, mu_par] for mu_par in model.mu_params], indexing='ij'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Interpolation grid in mu-space\n",
    "zoom = 8\n",
    "num_mu_igrid = {key: zoom*n for key, n in num_mu_grid.items()}\n",
    "\n",
    "mu_igrid, mu_igrid_1d = OrderedDict(), OrderedDict()\n",
    "mu_igrid_2d_params, mu_igrid_2d = OrderedDict(), OrderedDict()\n",
    "for model in models:\n",
    "    for mu_par in model.mu_params:\n",
    "        mu_igrid_1d[model.name, mu_par] = np.linspace(*model.mu_bounds[mu_par],\n",
    "                                                          num=num_mu_igrid[model.name, mu_par])\n",
    "    mu_igrid.update(OrderedDict(zip(\n",
    "        ((model.name, mu_par) for mu_par in model.mu_params),\n",
    "        np.meshgrid(*[mu_igrid_1d[model.name, mu_par] for mu_par in model.mu_params], indexing='ij'))))\n",
    "\n",
    "    # Will need subsets of 2 for plotting later\n",
    "    mu_igrid_2d_params[model.name] = [(x_par, y_par) for i, x_par in enumerate(model.mu_params) \n",
    "                                          for y_par in model.mu_params[i+1:]]\n",
    "    for xy_pars in mu_igrid_2d_params[model.name]:\n",
    "        mu_igrid_2d[model.name, xy_pars] = dict(zip(\n",
    "            xy_pars, np.meshgrid(*[mu_igrid_1d[model.name, mu_par] for mu_par in xy_pars], indexing='ij')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the data likelihood $P({\\rm data \\mid model}, \\mu)$\n",
    "$$\\begin{aligned}\n",
    "    P({\\rm data \\mid model}, \\mu) &\\propto \\prod_{i \\in \\rm\\{events\\}} \\int {\\rm d}p \\mathcal{L}_i(p) r(p, \\mu) \\\\\n",
    "    &= \\frac{\\prod_i \\int {\\rm d}p \\mathcal{L}_i(p) R(p, \\mu) V(p)} {\\left(\\int {\\rm d}p~R(p, \\mu) V(p) \\right)^{N_{\\rm events}}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "The $p$-integrals in the numerator are performed over each event's $p$-grid (where $\\mathcal{L}_i(p) \\neq 0$), the one in the denominator is over all $p$-space. We compute the denominator first:\n",
    "\n",
    "### Total detector-rate $\\overline\\lambda(\\mu)$\n",
    "\n",
    "If $R(\\mu, p)$ depends only in some of the $p$-parameters, $p^{(1)}$, and not on $p^{(2)}$, the total rate becomes:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\overline\\lambda(\\mu) &= \\int dp R(\\mu, p)V(p) \\\\\n",
    "    &= \\int dp^{(1)} R(\\mu, p^{(1)})\\int dp^{(2)} V(p^{(1)}, p^{(2)}) \\\\\n",
    "    &= \\int dp^{(1)} R(\\mu, p^{(1)}) \\overline V_2(p^{(1)}).\n",
    "\\end{aligned}$$\n",
    "\n",
    "We should compute $\\overline V_2(p^{(1)})$ and $R(\\mu, p^{(1)})$ separately over a $p^{(1)}$ grid, exploiting that:\n",
    "* $\\overline V_2$ does not depend on $\\mu$.\n",
    "*  $R$ does not depend on the PSD and approximant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    p1_params = model.p_params\n",
    "    p2_params = [p_par for p_par in p_params if not p_par in p1_params]\n",
    "    p1_igrid = OrderedDict(zip(p1_params, np.meshgrid(*[p_igrid_1d[par] for par in p1_params], indexing='ij')))\n",
    "    # Compute V2 = \\int V(p1,p2) dp2\n",
    "    V2 = {}\n",
    "    for psd in PSDs:\n",
    "        for approximant in approximants:\n",
    "            V = p_igrid[psd, approximant, 'V']\n",
    "            V2[psd, approximant] = V.sum(axis=tuple(p_params.index(par) for par in p2_params))\n",
    "    # Compute, on the mu_grid, the detector rate averaged over the p_grid\n",
    "    tot_det_rate = {PA: [] for PA in PSDs_approximants}\n",
    "    for mu_values in np.array([mu_grid[model.name, mu_par].flatten() for mu_par in model.mu_params]).T:\n",
    "        R = model(**p1_igrid, **dict(zip(model.mu_params, mu_values)))  # Physical rate\n",
    "        for PA in PSDs_approximants:\n",
    "            tot_det_rate[PA].append((R * V2[PA]).sum())\n",
    "    for PA in PSDs_approximants:  # Reshape and store tot_det_rate\n",
    "        mu_grid[model.name, 'tot_det_rate', (*PA)] = np.reshape(\n",
    "            tot_det_rate[PA], mu_grid[model.name, model.mu_params[0]].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $P({\\rm data \\mid model}, \\mu)$\n",
    "Now we compute the numerator terms $\\int {\\rm d}p \\mathcal{L}_i(p) R(p, \\mu) V(p)$, and the total $P({\\rm data \\mid model}, \\mu)$ normalized to $\\int {\\rm d}\\mu P = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    numerator = {PCA: np.ones_like(mu_grid[model.name, model.mu_params[0]])\n",
    "                 for PCA in PSDs_coherences_approximants}\n",
    "    for event in events:\n",
    "        numerator_i = {PCA: [] for PCA in PSDs_coherences_approximants}\n",
    "        for mu_values in np.array([mu_grid[model.name, mu_par].flatten() for mu_par in model.mu_params]).T:\n",
    "            R = model(**p_grid[event], **dict(zip(model.mu_params, mu_values)))  # Physical rate\n",
    "            for PCA in PSDs_coherences_approximants:\n",
    "                CA = PCA[1:]\n",
    "                PA = PCA[::2]\n",
    "                numerator_i[PCA].append(np.sum(R * p_grid[event]['likelihood_' + '_'.join(CA)]\n",
    "                                               * p_grid[event]['V_' + '_'.join(PA)]))\n",
    "        for PCA in PSDs_coherences_approximants:\n",
    "            numerator[PCA] *= np.reshape(\n",
    "                numerator_i[PCA], mu_grid[model.name, model.mu_params[0]].shape)\n",
    "    for PCA in PSDs_coherences_approximants:\n",
    "        PA = PCA[::2]\n",
    "        P = (numerator[PCA] / mu_grid[model.name, 'tot_det_rate', (*PA)]**len(events))\n",
    "        mu_grid[model.name, 'P', (*PCA)] = P / P.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate \n",
    "... $P({\\rm data \\mid model}, \\mu)$ on a refined grid in $\\mu$-space, `mu_igrid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    for PCA in PSDs_coherences_approximants:\n",
    "        P = ndimage.zoom(np.reshape(\n",
    "            mu_grid[model.name, 'P', (*PCA)],\n",
    "            tuple(num_mu_grid[model.name, mu_par] for mu_par in model.mu_params)), zoom, order=1)\n",
    "        mu_igrid[model.name, 'P', (*PCA)] = P / P.sum()  # Normalize again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginalize\n",
    "\n",
    "... over some $\\mu$ variables, on the interpolation grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    # Compute 1d marginalized likelihoods P(data|model):\n",
    "    for i, x_par in enumerate(model.mu_params):\n",
    "        js = tuple(j for j in range(len(model.mu_params)) if j != i) # Marginalize over these axes\n",
    "        for PCA in PSDs_coherences_approximants:\n",
    "            mu_igrid_1d[model.name, 'P', x_par, (*PCA)] \\\n",
    "                = mu_igrid[model.name, 'P', (*PCA)].sum(axis=js)\n",
    "    # Compute 2d marginalized likelihoods\n",
    "    for xy_pars in mu_igrid_2d_params[model.name]:\n",
    "        for PCA in PSDs_coherences_approximants:\n",
    "            js = tuple(j for j, z in enumerate(model.mu_params) if not z in xy_pars) # Marginalize over these\n",
    "            mu_igrid_2d[model.name, xy_pars]['P', (*PCA)] = mu_igrid[model.name, 'P', (*PCA)].sum(axis=js)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find $\\mu$-parameter estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "median, bounds_estimate, err_estimate = {}, {}, {}\n",
    "for model in models:\n",
    "    for PCA in PSDs_coherences_approximants:\n",
    "        for mu_par in model.mu_params:\n",
    "            cumulative_P = UnivariateSpline(\n",
    "                mu_igrid_1d[model.name, mu_par],\n",
    "                np.cumsum(mu_igrid_1d[model.name, 'P', mu_par, (*PCA)]) - .5, s=0)\n",
    "            median[model.name, (*PCA), mu_par] = cumulative_P.roots()[0]\n",
    "            P_level = brentq(lambda level, P: P[P > level].sum()-.9, 0, 1, \n",
    "                             args=(mu_igrid_1d[model.name, 'P', mu_par, (*PCA)]))\n",
    "            P_spline = UnivariateSpline(mu_igrid_1d[model.name, mu_par], \n",
    "                                        mu_igrid_1d[model.name, 'P', mu_par, (*PCA)] - P_level, s=0)\n",
    "            roots = P_spline.roots()\n",
    "            bounds_estimate[model.name, (*PCA), mu_par] = np.array([roots[0], roots[-1]])\n",
    "            if mu_igrid_1d[model.name, 'P', mu_par, (*PCA)][0] > P_level:\n",
    "                bounds_estimate[model.name, (*PCA), mu_par][0] = mu_igrid_1d[model.name, mu_par][0]\n",
    "            if mu_igrid_1d[model.name, 'P', mu_par, (*PCA)][-1] > P_level:\n",
    "                bounds_estimate[model.name, (*PCA), mu_par][1] = mu_igrid_1d[model.name, mu_par][-1]\n",
    "            err_estimate[model.name, (*PCA), mu_par] = abs(median[model.name, (*PCA), mu_par]\n",
    "                                                           - bounds_estimate[model.name, (*PCA), mu_par])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find % confidence contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fractions = [.5, .9]  # Probability enclosed by contours\n",
    "fractions = sorted(fractions, reverse=True)  # So that the P levels are increasing\n",
    "levels = {}\n",
    "for model in models:\n",
    "    for PCA in PSDs_coherences_approximants:\n",
    "        for xy_pars in mu_igrid_2d_params[model.name]:\n",
    "            levels[model.name, (*PCA), xy_pars] = [brentq(\n",
    "                lambda level, P: P[P > level].sum()-fraction, 0, 1,\n",
    "                args=(mu_igrid_2d[model.name, xy_pars]['P', (*PCA)])) for fraction in fractions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def latex_val_err(v, e):\n",
    "    '''Gets a value and its uncertainty, returns a latex string\n",
    "    $v^{+ep}_{-em}$ with the significant figures given by the uncertainties.\n",
    "    v: float\n",
    "    e: [err_m, err_p]\n",
    "    '''\n",
    "    n_digits = max(0, *[int(np.ceil(-np.log10(e_))) for e_ in e])\n",
    "    r = lambda a, n: round(a, n) if n > 0 else int(round(a))\n",
    "    return '${}_{{-{}}}^{{+{}}}$'.format(r(v, n_digits), *[r(e_, n_digits) for e_ in e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.clf();\n",
    "for model in models:\n",
    "    n_cols = n_rows = len(model.mu_params)\n",
    "    for PCA in PSDs_coherences_approximants:\n",
    "        fig, ax = plt.subplots(n_rows, n_cols, figsize=(2.7 * n_cols, 2.7 * n_rows + .5));\n",
    "        if n_cols == 1: ax = [[ax]]\n",
    "        plt.suptitle('{}'.format(model.latex), size='large')\n",
    "\n",
    "        # Plot 2D likelihoods (off-diagonal in the plot grid)\n",
    "        for row, y_par in list(enumerate(model.mu_params))[1:]:\n",
    "            for col, x_par in list(enumerate(model.mu_params))[:row]:\n",
    "                plt.sca(ax[row][col])\n",
    "                xy_P = mu_igrid_2d[model.name, (x_par, y_par)]['P', (*PCA)].T\n",
    "\n",
    "                plt.imshow(xy_P, extent=[*model.mu_bounds[x_par], *model.mu_bounds[y_par]],\n",
    "                           cmap='Oranges', origin='lower', aspect='auto', alpha=.8)\n",
    "\n",
    "                contours = plt.contour(mu_igrid_1d[model.name, x_par],\n",
    "                                       mu_igrid_1d[model.name, y_par],\n",
    "                                       xy_P,\n",
    "                                       levels=levels[model.name, (*PCA), (x_par, y_par)],\n",
    "                                       colors=['tab:orange', 'tab:brown'])\n",
    "                for i in range(len(fractions)):\n",
    "                    contours.collections[i].set_label('{:.0f}% c.l.'.format(100*fractions[i]))\n",
    "\n",
    "        # Plot 1D likelihoods (diagonal)\n",
    "        for i, mu_par in enumerate(model.mu_params):\n",
    "            plt.sca(ax[i][i])\n",
    "            for val in [median[model.name, (*PCA), mu_par], *bounds_estimate[model.name, (*PCA), mu_par]]:\n",
    "                plt.plot([val]*2, [0, np.interp(val, mu_igrid_1d[model.name, mu_par],\n",
    "                                                mu_igrid_1d[model.name, 'P', mu_par, (*PCA)])],\n",
    "                          'C0', alpha=.5)\n",
    "            span = np.linspace(*bounds_estimate[model.name, (*PCA), mu_par])\n",
    "            plt.fill_between(span, 0, np.interp(span, mu_igrid_1d[model.name, mu_par],\n",
    "                                                mu_igrid_1d[model.name, 'P', mu_par, (*PCA)]), alpha=.1)\n",
    "            \n",
    "            plt.title('{}$=${} {}'.format(\n",
    "                latex[mu_par],\n",
    "                latex_val_err(median[model.name, (*PCA), mu_par],\n",
    "                              err_estimate[model.name, (*PCA), mu_par]),\n",
    "                unit[mu_par].replace('(', '').replace(')','')))\n",
    "\n",
    "            plt.plot(mu_igrid_1d[model.name, mu_par],\n",
    "                     mu_igrid_1d[model.name, 'P', mu_par, (*PCA)], \n",
    "                     label=r'$P(\\{d_i\\} \\mid \\mathbf{\\mu})$')\n",
    "\n",
    "        # Embellish\n",
    "        for col, x_par in enumerate(model.mu_params):\n",
    "            ax[n_rows-1][col].set_xlabel(latex[x_par] + unit[x_par], size='large')\n",
    "            plt.setp(ax[n_rows-1][col].get_xticklabels(), rotation=45)\n",
    "            for row in range(n_rows-1):\n",
    "                ax[row][col].tick_params(labelbottom='off')\n",
    "        for row, y_par in list(enumerate(model.mu_params))[1:]:\n",
    "            ax[row][0].set_ylabel(latex[y_par] + unit[y_par], size='large')\n",
    "            plt.setp(ax[row][0].get_yticklabels(), rotation=45)\n",
    "            for col in range(1, n_cols):\n",
    "                ax[row][col].tick_params(labelleft='off')\n",
    "        for row in range(n_rows-1):\n",
    "            for col in range(row+1, n_cols):\n",
    "                ax[row][col].axis('off')\n",
    "        for col in range(n_cols):\n",
    "            for row in range(1, n_rows):\n",
    "                ax[0][col].get_shared_x_axes().join(ax[0][col], ax[row][col])\n",
    "                ax[row][col].autoscale()\n",
    "        for row in range(n_rows):\n",
    "            for col in range(1, row):\n",
    "                ax[row][0].get_shared_y_axes().join(ax[row][0], ax[row][col])\n",
    "                ax[row][col].autoscale()\n",
    "        for i in range(len(model.mu_params)):\n",
    "            ax[i][i].tick_params(axis='y', left='off', labelleft='off')\n",
    "            ax[i][i].set_ylim(ymin=0)\n",
    "        ax[-1][-1].set_xlim(model.mu_bounds[model.mu_params[-1]])\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96]);\n",
    "        plt.subplots_adjust(hspace=0.04, wspace=0.04);\n",
    "\n",
    "        handles_1d, labels_1d = ax[0][0].get_legend_handles_labels()\n",
    "        try:\n",
    "            handles_2d, labels_2d = ax[1][0].get_legend_handles_labels()\n",
    "            pos = ax[0][1].get_position()\n",
    "            fig.legend(handles_1d + handles_2d, labels_1d + labels_2d, \n",
    "                       loc='upper left', bbox_to_anchor=(pos.x0, pos.y1), frameon=False)\n",
    "        except IndexError:\n",
    "            ax[0][0].legend(handles_1d, [l.replace('data', 'd').replace('model', 'm') for l in labels_1d],\n",
    "                            loc='best', frameon=False)\n",
    "\n",
    "        plt.savefig('figures/{}_{}.pdf'.format(model.name, '_'.join(PCA)), bbox_inches='tight')\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
